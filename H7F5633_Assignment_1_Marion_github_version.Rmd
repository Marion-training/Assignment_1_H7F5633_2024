---
title: "Bioinformatics Analysis and Visualisation of Medical Genomics Data - H7F5633
  - Assignment 1"
author: "Marion"
date: "2024-09-19"
output:
  github_document:
    html_preview: false
---

# Task 1 - Literature

## 1.

Group 6 --> Publication: "Single-cell analyses and host genetics highlight the role of innate immune cells in COVID-19 severity"
Edahiro et al., Nat Genet, 2023


## 2.

### a. Medically relevant insight from the article:
Monocytes or dendritic cells are dysfunctional in patients with severe COVID-19. Of note, the monocyte dysfunction is associated with a particular variant of the gene IFNAR2, encoding the receptor for IFN-I, in monocytes. In addition, there is an involvement of host genetic risk of severe COVID-19 with innate immunity.

### b. Genomics technology that were used:
- 10 X chromium was used for the single-cell transcriptional profiling (scRNAseq) 
- 10 X chromium was also used for the sequencing of T cell receptors (TCR) and B cell receptors (BCR) to analyze the T cell and B cell repertoires
- SNP genotyping was used to analyze the host genetics data


## 3.

### a. Questions that extend the analysis presented in the paper:
- What are the mechanisms involved in the reduced differentiation from classical to non-classical monocytes (ncMono) in patients with severe COVID-19?
- What are the mechanisms involved in the dysfunction of monocytes in patients with severe COVID-19?
- Could the reduced interaction between CXCL10 and CXCR3 be one of the factors responsible for driving COVID-19 severity?


# Task 2 - Git repositories and R Markdown

--> creation of a GitHub account 

# Task 3 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# load the tidyverse and ggplot2 packages that are necessary for subsequent tasks
library(tidyverse)
library(ggplot2)
```
Note: I cannot install (and load) the Bioconductor package because the latest version of Bioconductor is incompatible with the R version that I am currently using (4.4.1). To use it, I would need to downgrade to the 4.4.0 R version...

# Task 4 - Using R example data sets

## 1 and 2. Using the function help() to look at the content of the "CO2" dataset.
```{r}
help("CO2")
```
Here is the description of the R internal dataset "CO2" (Carbon Dioxide Uptake in Grass Plants): 
"The CO2 data frame has 84 rows and 5 columns of data from an experiment on the cold tolerance of the grass species Echinochloa crus-galli.". It is an object of class c("nfnGroupedData", "nfGroupedData", "groupedData", "data.frame").

## 3. Code to calculate the average and median CO2 uptake of the plants from Quebec and Mississippi:
```{r}
# Creates a vector which contains the characters "Quebec" and "Mississippi" by taking the levels of the factor "Type" (one of the columns of the CO2 dataset)
regions <- levels(CO2$Type)
# Creates a loop so that the actions will be done for each of the characters in the vector "regions"
for (i in regions) {
  # calculates the mean of the values in the column "uptake" from the CO2 dataset, only for the rows which have "i" as a "Type"
  average <- mean(CO2$uptake[CO2$Type==i])
  # prints "average CO2 uptake of the plants from" + i + "=" + the calculated average
  print (paste("average CO2 uptake of the plants from", i, "=", average))
  # calculates the median of the values in the column "uptake" from the CO2 dataset, only for the rows which have "i" as a "Type"
  median <- median(CO2$uptake[CO2$Type==i])
  # prints "median CO2 uptake of the plants from" + i + "=" + the calculated median
  print (paste("median CO2 uptake of the plants from", i, "=", median))
}
```

# Task 5 - R functions

# 1. Code to create a function that calculates the ratio of the mean and the median of a given vector
```{r}
# Creates a function ratio() that calculates the ratio of the mean and the median for "vector", using the function function()
ratio <- function(vector) {
  # Calculates the mean of the vector
  x <- mean(vector)
  # Calculates the median of the vector
  y <- median(vector)
  # calculates the ratio of the mean and median and returns this as a result of the function ratio()
  return(x/y)
}
# Creates a vector "a" to test the function ratio()
a <- c(1, 3, 7, 11)
# test the function ratio
ratio(a)
```

## 2. Code to create a function that ignores the lowest and highest values from a given a vector and calculate the mean
```{r}
# Creates a function mean_vector() that calculates the mean of a given vector after removing the highest and lowest values of this vector
mean_vector <- function(vector) {
  # Finds the highest value from the vector
  max <- max(vector)
  # Finds the lowest value from the vector
  min <- min(vector)
  # Creates a filtered vector by removing the highest and lowest values from the initial vector
  filtered_vector <- vector[!vector %in% c(min, max)]
  # calculates the mean of the filtered vector and returns this as a result of the function mean_vector()
  return(mean(filtered_vector))
}
# Creates a vector "a" to test the function ratio()
a <- c(1, 3, 7, 11)
# test the function mean_vector()
mean_vector(a)
```


## 3. Piping

https://r4ds.had.co.nz/pipes.html#pipes

Why, how and when not to use pipes: 
Pipes simplify complex data manipulations by chaining operations, enhancing code readability and reducing intermediate object creation. The %>% operator allows for intuitive left-to-right function composition. Pipes are ideal for linear sequences but less suitable for complex dependency structures or when dealing with multiple inputs/outputs. Pipes are are not recommended for pipes longer than ten steps (it is better to use intermediate objects in that case). And finally, they don't work with functions that use the current environment or rely on lazy evaluation. 

## 4. Apply-family of functions

http://uc-r.github.io/apply_family

Why the apply-family of functions can be useful in my work: 
The apply-family of functions, including apply(), lapply(), sapply(), and tapply(), apply a given function to a data object and can replace loops (in certain cases), which allows for a code that is more compact and easier to read. I could use this family of functions for both statistical analysis and visualization of my datasets, particularly the apply() function, to apply a function to each row or column of my matrices. It would improve my codes a lot and avoid potential errors in the analysis.

# Task 6 - Basic visualization with R

## 1. Code to compare the distributions of the body heights of the two species from the "magic_guys.csv" dataset graphically
### a. Histograms
```{r}
# read the csv file "magic_guys.csv"
magic_guys <- read_csv("~/Desktop/SciLifeLab_Riken/magic_guys.csv")
body_heights <- magic_guys$length
species <-  unique(magic_guys$species)

# Creates a vector that assign the groups (species) to a color
colors <- setNames(c("sandybrown", "cadetblue"), species)

# Creates a vector containing the values of the x axis where the breaks happen
# It takes the maximum and minimum values of the "body_heights" and requires 10 breaks
my_breaks <- seq(min(body_heights), max(body_heights), length.out = 10)

# Creates an empty plot
plot(NA, xlim = range(body_heights), ylim = c(0, 20), 
     xlab = "Body height (cm)", ylab = "Frequency", 
     main = "Distributions of the body heights in different species - Base R")

# Loop to create an histogram (with 30 breaks) for each group (species) and add it to the empty plot
for (i in species) {
  current_data <- magic_guys %>% 
    filter(species == i)
  body_heights <- current_data$length
  hist(body_heights, 
       col = colors[i], 
       border = "black", 
       add = TRUE, 
       breaks = my_breaks)
}

# Adds a legend to the plot
legend("topright", legend = unique(species), fill = colors, title = "Species")

# Same histogram but with 40 breaks instead of 10
body_heights <- magic_guys$length
species <-  unique(magic_guys$species)
colors <- setNames(c("sandybrown", "cadetblue"), species)
my_breaks <- seq(min(body_heights), max(body_heights), length.out = 40)
plot(NA, xlim = range(body_heights), ylim = c(0, 7), 
     xlab = "Body height (cm)", ylab = "Frequency", 
     main = "Distributions of the body heights in different species - Base R")
for (i in species) {
  current_data <- magic_guys %>% 
    filter(species == i)
  body_heights <- current_data$length
  hist(body_heights, 
       col = colors[i], 
       border = "black", 
       add = TRUE, 
       breaks = my_breaks)
}
legend("topright", legend = unique(species), fill = colors, title = "Species")

# The problem with these plots is that there is no transparency so we can't see the overlap...
```

#### Using ggplot() 
```{r}
body_heights <- magic_guys$length
species <-  unique(magic_guys$species)

# creates a histogram with the two groups in different colors, with 40 breaks (bins)
# The colors of the groups and the titles (plot, axis and legend) are customized
# The bars are semi-transparents to see a potential overlap of the two histograms
histo_gg <- ggplot(magic_guys, aes(x = body_heights, fill = as.factor(species))) +
# the argument "identity" og geom_histogram() allows to have two separate histograms instead of having only one and the argument apha allows to change the transparency so that we can see the overlap (instead of one histogram masking the other)
geom_histogram(color = "black", bins = 40, alpha = 0.5, position = "identity") +
scale_fill_manual(values = c("sandybrown", "cadetblue"), name = "Species") +
theme_classic() +
labs(title = "Distributions of the body heights in different species - Gggplot", x = "Body height (cm)", y = "Frequency") 

print(histo_gg)
```

### b. Boxplots
```{r}
body_heights <- magic_guys$length
species <-  unique(magic_guys$species)

box_gg <- ggplot(magic_guys, aes(x = species, y = body_heights, fill = as.factor(species))) +
  geom_boxplot(width = 0.5, color = "black") +
  scale_fill_manual(values = c("sandybrown", "cadetblue"), name = "Species") +
  theme_classic() +
  labs(title = "Distributions of the body heights in different species - Boxplot", x = NULL, y = "Body height (cm)") 

print(box_gg)
```

### c. Saving plots in various formats
```{r}
# choosing the working directory where the files will be saved
setwd("~/Desktop/SciLifeLab_Riken/")

# opens a png file and names it
png("boxplot.png")

# print the boxplot
print(box_gg)

# close the file
dev.off()

# Same with pdf file
pdf("boxplot.pdf")
print(box_gg)
dev.off()

# Same with svg file
svg("boxplot.svg")
print(box_gg)
dev.off()

# Other attempts to save a svg file... still doesn't work
ggsave("boxplot.svg", plot = box_gg)

# Other attempts to save a svg file... Now loading the "svglite" package (after having installed it on the console) o) as it seems it was missing
library(svglite)
ggsave("boxplot.svg", plot = box_gg)
```
Notes: 
- I got multiple errors before I managed to save as ".svg" file. I solved this by installing and loading the package "svglite"
- I get a problem when "knitting" if I write "install.packages("svglite") in the R chunk, so I have to write it only in the console.

I would use .png for presentations (import in ppt), .svg for further editing (import in illustrator files) and .pdf 

## 2. Work with "microarray_data.tab"

### a.
```{r}
# Reads the microarray file (specify the separator as "\ t")
microarray <- read.table("~/Desktop/SciLifeLab_Riken/microarray_data.tab", header=TRUE, sep="\t")
# Calculates number of columns
cols <- ncol(microarray)
# Calculates number of rows
rows <- nrow(microarray)
print(paste("The", "matrix", "has", cols, "columns", "and", rows, "rows"))
```

### b.
```{r}
# Uses is.na() function to identify the missing values and uses colSums() to calculate this for each column
na_count <- colSums(is.na(microarray))
# print the 10 first values of "na_count"
print(head(na_count, 10))

hist(na_count, 
     border = "black", 
     breaks = cols,
     xlim = range(na_count), 
     ylim = c(0, 25),
     xlab = "number of missing values per gene", 
     ylab = "Frequency", 
     main = "Distributions of the number of missing values per gene"
     )
```

### c.
```{r}
# creates a new function to calculate the percentage of

na_count_df <- as.data.frame(na_count)

calcul_percent <- function(x) {
        return(x/rows*100)
    }

# Apply this new function of all elemnts of the vector na_count
na_count_df$percent <- sapply(na_count_df$na_count, function(x) calcul_percent(x))

na_count_df$gene_name <- row.names(na_count_df)

# creates a new function to detect which genes have more than 50% or 20% or 10% missing values
missing_val <- function(w, x, y, z) {
  if (x > y) {
    z <- c(z, w)
  }
  return(z)
}

# Creates a vector with the different percentages of missing values to be tested (10, 20 and 50%)
cut_off <- c(10, 20, 50)  
genes <- na_count_df$gene_name

# Creates a loop to find the genes with missing values for the cut-offs contained in the vector "cut_off"
for (y in cut_off) {
  # Creates an empty vector, which will later be filled with the names of genes
  z <- c()
  # Creates a loop so that the empty vector is filled for the genes with a % value greater than y
  for (w in genes) {
    x <- na_count_df$percent[na_count_df$gene_name == w]
    z <- missing_val(w, x, y, z)
  }
  # If the vector containing the genes with a % value greater than y are longer than 0 (meaning there is at least 1 gene that is contained in the vector), it will print a sentence along with the names of those genes
  if (length(z) > 0) {
    print(paste("Genes with greater than", y, "% missing values:", paste(z, collapse=", ")))
  } else {
    print(paste("No genes with greater than", y, "% missing values"))
  }
}
```

### d.
```{r}
# Calculate the mean of each columns of "microarray", while ignoring NAs
mean_result <- colMeans(microarray, na.rm = TRUE)

# create a vector of gene names by taking the column names of "micrpoarray"
microarray_genes <- colnames(microarray)

#print "microarray" before changing the NA values
print(head(microarray[, 1:10], 10))

# replace NA with the average expression value for the particular gene using a loop (for each gene)
for (gene in microarray_genes) {
  microarray[is.na(microarray[,gene]), gene] <- mean_result[gene]
}

#print "microarray" after this change
print(head(microarray[, 1:10], 10))
```

## 3. Visualize the data in CO2 dataset in a way that gives you a deeper understanding of the data

```{r}
# Plot the CO2 uptake as a function of the concentration, with different colors for "Treatment" and different line type for the "Type"
ggplot(CO2, aes(x = conc, y = uptake, color = Treatment, linetype = Type)) +
  geom_point() +
  # add a trend line
  geom_smooth(method = "loess", se = FALSE) +
  labs(x = "CO2 Concentration", 
       y = "CO2 Uptake",
       title = "CO2 Uptake as a function of the concentration") +
  # Different colors depending on Treatment
  scale_color_manual(values = c("darkgreen", "darkmagenta")) + 
  # Different line types depending on Type
  scale_linetype_manual(values = c("dotted", "solid")) + 
  theme_classic()
```

What I see:
- CO2 uptake is higher in non-chilled compared to chilled treatment
- CO2 uptake is higher in Quebec compared to Mississippi
- CO2 uptake seems to reach a plateau after a certain concentration

# Task 7 

## 1. Install Tidybiology package, using: devtools::install_github("hirscheylab/tidybiology")

```{r}
# loading the "tidybiology" package (after installing it)
library(tidybiology)
# printing the "chromosome" dataset
print(head(chromosome, 5))
# printing the "proteins" dataset
print(head(proteins, 5))
```

### a. 
```{r}
# Creates a matrix to be filled with summary data
data_summary <- matrix(NA, nrow = 3, ncol = 3)

# Set the names for the columns and rows of this matrix
colnames(data_summary) <- c("variations", "protein_codinggenes", "mi_rna")
rownames(data_summary) <- c("mean", "median", "maximum")

# Print the empty matrix
print(data_summary)

# Create a loop so that the statistics are calculated for the three variables of interest
for (v in colnames(data_summary)) {
  # Use the tidyverse to select the column of the variable of interest from the chromosome dataset
  current_data <- chromosome %>% 
    select(v)  %>% 
    # convert this data into a vector
    unlist()
 
# Fill the matrix with the summary data  
data_summary["mean", v] <- mean(current_data)
data_summary["median", v] <- median(current_data)
data_summary["maximum", v] <- max(current_data)
}

print(data_summary)
```

### b. 
```{r}
# Creates an histogram using ggplot
distrib <- ggplot(chromosome, aes(x = length_mm)) +  
geom_histogram(color = "deeppink4", fill = "deeppink", bins =50) +
theme_classic() +
labs(title = "Distribution of chromosome size", x = "chromosome size (mm)", y = "Frequency") 

print(distrib)
```

### c. 
```{r}
# Plot to visualize a potential correlation between the number of protein coding genes and the length of the chromosome
blue <- ggplot(chromosome, aes(x = length_mm, y = protein_codinggenes)) +
  geom_point(size = 1, color = "black") +
  labs(x = "Chromosome length (mm)", 
       y = "Number of protein coding genes",
       title = "Number of protein coding genes as a function of chromosome length") +
  theme_classic() +
  # Add a line to visualize a linear regression
  geom_smooth(method = "lm", se = FALSE, color = "blue", size = 0.5)

print(blue)


# Plot to visualize a potential correlation between the number of miRNAs and the length of the chromosome
red <- ggplot(chromosome, aes(x = length_mm, y = mi_rna)) +
  geom_point(size = 1, color = "black") +
  labs(x = "Chromosome length (mm)", 
       y = "Number of miRNAs",
       title = "Number of miRNAs as a function of chromosome length") +
  theme_classic() +
  # Add a line to visualize a linear regression
  geom_smooth(method = "lm", se = FALSE, color = "red", size = 0.5)

print(red)

```

Outcome: Both the number of protein coding genes and the number of miRNAs correlate with the chromosome length

### d. 
```{r}
summarized_data <- matrix(NA, nrow = 3, ncol = 2)

# Set the names for the columns and rows of this matrix
colnames(summarized_data) <- c("length", "mass")
rownames(summarized_data) <- c("mean", "median", "maximum")

# Create a loop so that the statistics are calculated for the three variables of interest
for (v in colnames(summarized_data)) {
  # Use the tidyverse to select the column of the variable of interest from the chromosome dataset
  current_data <- proteins %>% 
    select(v)  %>% 
    # convert this data into a vector
    unlist()
 
# Fill the matrix with the summarized data  
summarized_data["mean", v] <- mean(current_data)
summarized_data["median", v] <- median(current_data)
summarized_data["maximum", v] <- max(current_data)
}

print(summarized_data)

#
length_vs_mass <- ggplot(proteins, aes(x = length, y = mass)) +
  geom_point(size = 3, color = "navyblue", fill = "mediumturquoise", shape = 21) +
  labs(x = "Protein length", 
       y = "Protein mass",
       title = "Protein mass as a function of protein length") +
  theme_classic() +
  geom_smooth(method = "lm", se = FALSE, color = "navyblue", size = 1)

print(length_vs_mass)
```

###